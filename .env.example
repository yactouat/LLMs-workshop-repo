# LLM Provider Configuration
# Set to "ollama" (default) or "google"
LLM_PROVIDER=ollama

# Ollama Configuration (for LLM_PROVIDER=ollama)
# Optional: Override auto-detected model (e.g., "qwen3", "lama3.1")
OLLAMA_MODEL=llama3.1

# Optional: Specify which Ollama model supports thinking/reasoning
# Reasoning traces are enabled when --thinking flag is used and model matches this value
# Example: OLLAMA_THINKING_MODEL=qwen3
OLLAMA_THINKING_MODEL=qwen3

# Google AI Studio Configuration (for LLM_PROVIDER=google)
# Required: Your Google AI Studio API key
# Get one at: https://ai.google.dev/
GOOGLE_API_KEY=your_api_key_here

# Optional: Override default Google model (default: gemini-3-flash-preview)
GOOGLE_MODEL=gemini-3-flash-preview

# Optional: Specify which Google model supports thinking/reasoning
# Reasoning traces are enabled when --thinking flag is used and model matches this value
# Example: GOOGLE_THINKING_MODEL=gemini-3-flash-preview
GOOGLE_THINKING_MODEL=gemini-3-flash-preview
