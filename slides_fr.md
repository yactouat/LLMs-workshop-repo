# DevFest 2025 : LLMs Locaux, RAG et Architectures Multi-Agents

Construire le Cerveau Interne de ACME Corp

---

## √Ä mon propos

[Yacine Touati](https://www.linkedin.com/in/yactouat/)

D√©veloppeur logiciel et futuriste optimiste. Passionn√© par la tech, l'IA, le d√©veloppement web et la Culture. Construction et enseignement de syst√®mes d'information. üöÄ

Travaille actuellement chez Lamalo, une startup bas√©e √† Strasbourg, qui d√©veloppe une application d'agr√©gateur d'IAs am√©lior√©.

---

## La Mission

**Objectif :** Construire un syst√®me intelligent pour "ACME Corp" avec des options de d√©ploiement flexibles.

**Options de Fournisseur :**
- **Local (Ollama) :** Confidentialit√© d'abord, hors ligne, z√©ro co√ªt d'API
  - Fonctionne sur votre mat√©riel (8GB+ RAM recommand√©)
  - Mod√®les Llama 3.1 / Qwen 3
  - Souverainet√© compl√®te des donn√©es
- **Cloud (Google AI Studio) :** D√©marrage rapide, pas de configuration locale
  - Utilise le mod√®le Gemini 3 Flash Preview
  - N√©cessite une cl√© API et une connexion internet

**La Stack :**
- **Moteur :** Ollama (local) OU Google AI Studio (cloud) - configurable via LLM_PROVIDER
- **Orchestration :** LangChain (Le Squelette) & LangGraph (Le Cerveau)
- **M√©moire :** SQLite (Vectorielle avec l'extension VSS)

---

## D√©mo 1 : Hello World avec LLM

**Commande :** `python3 01_local_llm/hello_world.py [--thinking]`

**Ce Que Vous Verrez :**
- Interaction directe avec un LLM (Ollama local OU Google cloud)
- Le mod√®le tente de r√©pondre : "Qui est le PDG de ACME Corp ?"
- Le mod√®le ne peut pas r√©pondre car ACME Corp n'est pas dans ses donn√©es d'entra√Ænement

**S√©lection du Fournisseur :**
- Par d√©faut : Ollama (local)
- Option cloud : D√©finissez `LLM_PROVIDER=google` dans le fichier `.env` avec `GOOGLE_API_KEY`

**Qu'est-ce Que les Mod√®les "Thinking" ?**
- Ajoutez le flag `--thinking` pour voir le processus de raisonnement du mod√®le
- Le mod√®le expose sa cha√Æne de pens√©e interne avant de r√©pondre
- Ollama : Utilise qwen3 (configur√© via OLLAMA_THINKING_MODEL)
- Google : Utilise gemini-3-flash-preview (configur√© via GOOGLE_THINKING_MODEL)

**Le Probl√®me :**
Les LLMs ont une date limite de connaissances fixe. Ils ne peuvent pas r√©pondre aux questions sur des informations priv√©es ou des √©v√©nements r√©cents.

**La Solution :** RAG (G√©n√©ration Augment√©e par R√©cup√©ration)

---

## D√©mo 2a : Ingestion RAG - Construction de la Base de Connaissances

**Commande :** `python3 02_rag_lcel/ingest.py`

**Ce Qui Se Passe :**
1. **Charger** la base de connaissances (politiques d'entreprise, info PDG, culture)
2. **D√©couper** le document s√©mantiquement
3. **Vectoriser** chaque morceau en vecteurs (embeddings sp√©cifiques au fournisseur)
4. **Stocker** les vecteurs dans SQLite avec l'extension VSS

**Embeddings par Fournisseur :**
- Ollama : nomic-embed-text
- Google : gemini-embedding-001

**Important :** Relancez `ingest.py` lors du changement de fournisseur - les embeddings ne sont pas compatibles !

**Pourquoi le RAG ?**
- Les LLMs ont du mal avec les grands contextes (10k+ tokens)
- RAG : Ne r√©cup√®re que ce qui est pertinent (top 3-5 morceaux)

---

## Qu'est-ce Que les Embeddings Vectoriels ?

**Texte ‚Üí Nombres :**
- Les mots et phrases deviennent des vecteurs dans un espace de haute dimension
- Significations similaires = Vecteurs similaires (math√©matiquement)

**Exemple :**
```
"roi" - "homme" + "femme" ‚âà "reine"
```

**Pourquoi les Embeddings Importent :**
- Permettent la **recherche s√©mantique** (par sens, pas seulement par mots-cl√©s)
- "PDG" trouve "directeur g√©n√©ral" sans correspondance exacte
- Fondement de la recherche IA moderne et du RAG

---

## D√©mo 2b : Requ√™te RAG - LCEL en Action

**Commande :** `python3 02_rag_lcel/query.py [--interactive] [--question "VOTRE_QUESTION"] [--thinking]`

**LCEL = Langage d'Expression LangChain**

Pensez **pipes Linux** pour l'IA :
```python
chain = retriever | prompt | model | output_parser
```

**Comment √áa Marche :**
1. Question ‚Üí embedding vectoriel (utilise le m√™me fournisseur que ingest.py)
2. R√©cup√©ration des 5 morceaux les plus similaires depuis SQLite
3. Contexte + question ‚Üí LLM (Ollama ou Google)
4. Le LLM g√©n√®re la r√©ponse

**Pourquoi LCEL ?**
- Composable (mixer les composants comme des LEGO)
- Lisible (flux de donn√©es clair)
- Optimis√© (parall√©lisation automatique)
- Agnostique du fournisseur (fonctionne avec Ollama et Google)

**Limitation :** S√©quentiel (A ‚Üí B ‚Üí C). Et si vous avez besoin de boucles ?

---

## D√©mo 3 : Agent ReAct LangGraph - Quand les Cha√Ænes Ne Suffisent Pas

**Commande :** `python3 03_langgraph_react/agent.py [--interactive] [--question "VOTRE_QUESTION"] [--thinking]`

**Cha√Æne vs. Graphe :**
| Cha√Æne (LCEL) | Graphe (LangGraph) |
|--------------|-------------------|
| S√©quentiel | Cyclique (boucles) |
| Pipeline fixe | D√©cisions dynamiques |
| Recette lin√©aire | Organigramme |

**ReAct = Raisonnement + Action :**
```
[D√âBUT] ‚Üí agent ‚Üí d√©cision?
                   ‚îú‚îÄ besoin_outil ‚Üí outils ‚Üí agent (boucle)
                   ‚îî‚îÄ termin√© ‚Üí [FIN]
```

**Innovation Cl√© :**
- L'agent **choisit** quel outil utiliser
- Peut boucler plusieurs fois
- Raisonnement avant l'action
- Fonctionne avec les mod√®les Ollama et Google

**Outils Disponibles :**
- `lookup_policy` : Interroger la base de connaissances (RAG - utilise les m√™mes embeddings que ingest.py)
- `search_tech_events` : Trouver des conf√©rences

**Note :** N√©cessite l'ex√©cution de `ingest.py` d'abord avec le m√™me fournisseur

---

## D√©mo 4 : Le Pattern Superviseur - Contr√¥le Centralis√©

**Commande :** `python3 04_supervisor/supervisor.py [--interactive] [--question "VOTRE_QUESTION"] [--thinking]`

**Topologie :** Centralis√©e (Hub and Spoke)

```
        Chercheur ‚Üê‚îÄ‚îê
              ‚Üë      ‚îÇ
Utilisateur ‚Üí Superviseur ‚Üí R√©dacteur
              ‚Üì      ‚îÇ
        V√©rificateur ‚îò
```

**Comment √áa Marche :**
1. Le superviseur re√ßoit la question
2. D√©l√®gue aux travailleurs sp√©cialis√©s (Chercheur, R√©dacteur, V√©rificateur)
3. Le travailleur retourne les r√©sultats
4. Le superviseur synth√©tise et r√©pond

**Support des Fournisseurs :**
- Fonctionne avec les mod√®les Ollama et Google
- Tous les agents utilisent le m√™me fournisseur (configur√© via LLM_PROVIDER)
- L'agent Chercheur utilise le RAG (n√©cessite des embeddings correspondants)

**Avantages :** Facile √† d√©boguer, d√©l√©gation claire, workflows orchestr√©s

**Inconv√©nients :** Goulot d'√©tranglement du superviseur, moins flexible

**Cas d'Usage :** Workflows pr√©visibles, t√¢ches hi√©rarchiques

**Note :** N√©cessite l'ex√©cution de `ingest.py` d'abord avec le m√™me fournisseur

---

## D√©mo 5 : Le Pattern R√©seau/Essaim - Collaboration D√©centralis√©e

**Commande :** `python3 05_network/network.py [--interactive] [--question "VOTRE_QUESTION"] [--thinking]`

**Topologie :** D√©centralis√©e (Maillage / Pair-√†-Pair)

```
Chercheur ‚Üê‚Üí R√©dacteur
    ‚Üï          ‚Üï
V√©rificateur ‚Üê‚Üí Coordinateur
```

**Comment √áa Marche :**
- Les agents communiquent **directement** entre eux
- N'importe quel agent peut passer la main √† un autre
- Pas de coordinateur central
- Collaboration auto-organis√©e
- Utilise les outils `transfer_to_*()` pour les passages pair-√†-pair

**Support des Fournisseurs :**
- Fonctionne avec les mod√®les Ollama et Google
- Tous les agents utilisent le m√™me fournisseur (configur√© via LLM_PROVIDER)
- L'agent Chercheur utilise le RAG (n√©cessite des embeddings correspondants)

**Avantages :** Flexible, pas de goulot, collaboration cr√©ative

**Inconv√©nients :** Plus difficile √† d√©boguer, risque de boucles, moins pr√©visible

**Cas d'Usage :** T√¢ches cr√©atives, recherche exploratoire

**Note :** N√©cessite l'ex√©cution de `ingest.py` d'abord avec le m√™me fournisseur

---

## Syst√®mes Multi-Agents Centralis√©s vs. D√©centralis√©s

**Centralis√© (Superviseur) :**
- Un agent contr√¥le le routage
- Topologie en √©toile
- Ex√©cution pr√©visible
- Exemple : Manager assigne les t√¢ches √† l'√©quipe

**D√©centralis√© (R√©seau/Essaim) :**
- Les agents se coordonnent en pair-√†-pair
- Topologie maill√©e
- Ex√©cution dynamique
- Exemple : L'√©quipe s'auto-organise les t√¢ches

**Diff√©rence Cl√© :** Qui d√©cide de la prochaine √©tape ?

---

## Comparaison Architecturale : Superviseur vs. R√©seau

| Aspect | Superviseur | R√©seau |
|--------|-----------|---------|
| **Contr√¥le** | Centralis√© | D√©centralis√© |
| **Routage** | Le manager d√©cide | Les agents s'auto-coordonnent |
| **Visibilit√©** | Facile √† tracer | Chemins complexes |
| **Flexibilit√©** | Structur√© | Hautement adaptatif |
| **Id√©al Pour** | Workflows pr√©visibles | Exploration cr√©ative |

**Hybride :** Combinez les deux (ex: Superviseurs par d√©partement, r√©seaux √† l'int√©rieur)

---

## Horizons Futurs : Au-del√† du RAG Vectoriel

**GraphRAG :**
- RAG standard : R√©cup√®re des morceaux de texte
- GraphRAG : R√©cup√®re les **relations** entre entit√©s
- Exemple : "Qui rend compte au PDG ?" n√©cessite de la structure, pas juste de la similarit√©

**Graphes de Connaissances :**
- Entit√©s + Relations (rend_compte_√†, d√©finit, impacte)
- Raisonnement sur des connaissances structur√©es
- Outils : Neo4j, triplets RDF, graphes de propri√©t√©s

**RAG Agentique :**
- Les agents interrogent les bases vectorielles **ET** les graphes de connaissances
- Capacit√©s de raisonnement plus sophistiqu√©es

---

## Points Cl√©s √† Retenir

**1. Options de D√©ploiement Flexibles**
- Local (Ollama) : Confidentialit√©, co√ªt z√©ro, fonctionnement hors ligne
- Cloud (Google) : D√©marrage rapide, pas d'exigences mat√©rielles
- Changement facile entre fournisseurs via LLM_PROVIDER

**2. Le RAG Comble les Lacunes de Connaissances**
- Les embeddings permettent la recherche s√©mantique
- SQLite VSS le rend accessible
- Embeddings sp√©cifiques au fournisseur (doivent correspondre entre ingest et query)

**3. Les Graphes Permettent l'Intelligence**
- LangGraph : prise de d√©cision adaptative
- ReAct : raisonnement + utilisation d'outils en boucles
- Fonctionne de mani√®re transparente avec les mod√®les locaux et cloud

**4. Les Syst√®mes Multi-Agents Passent √† l'√âchelle**
- Centralis√© (Superviseur) : workflows pr√©visibles
- D√©centralis√© (R√©seau) : collaboration cr√©ative
- Architecture agnostique du fournisseur

**5. Futur : GraphRAG + Architectures Hybrides**

---

## Ressources & Prochaines √âtapes

**D√©p√¥t de l'Atelier :**
https://github.com/yactouat/devfest-2025-local-llms-workshop

**Outils Essentiels :**
- Ollama : https://ollama.com
- LangChain : https://python.langchain.com
- LangGraph : https://langchain-ai.github.io/langgraph
- SQLite VSS : https://github.com/asg017/sqlite-vss

**Continuer l'Apprentissage :**
- **[LangChain Academy](https://academy.langchain.com/)** - Cours gratuits sur LangChain & LangGraph
- Exp√©rimentez avec les 5 d√©mos de cet atelier
- Ajoutez des checkpoints de m√©moire √† vos graphes
- Construisez votre propre base de connaissances
- Cr√©ez des outils personnalis√©s pour votre domaine

**Merci !**

*Maintenant allez construire quelque chose d'intelligent, priv√© et local.*
